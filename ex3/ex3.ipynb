{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% Machine Learning Online Class - Exercise 3 | Part 1: One-vs-all\n",
    "%  Instructions\n",
    "%  ------------\n",
    "%  This file contains code that helps you get started on the\n",
    "%  linear exercise. You will need to complete the following functions\n",
    "%  in this exericse:\n",
    "%     lrCostFunction.m (logistic regression cost function)\n",
    "%     oneVsAll.m\n",
    "%     predictOneVsAll.m\n",
    "%     predict.m\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.io import loadmat  \n",
    "import seaborn as sbs\n",
    "import matplotlib.image as mpimg\n",
    "import scipy.optimize as op\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% Setup the parameters you will use for this part of the exercise\n",
    "input_layer_size  = 400;  #% 20x20 Input Images of Digits\n",
    "num_labels = 10;          #% 10 labels, from 1 to 10\n",
    "                          #% (note that we have mapped \"0\" to label 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% =========== Part 1: Loading and Visualizing Data =============\n",
    "%  We start the exercise by first loading and visualizing the dataset.\n",
    "%  You will be working with a dataset that contains handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000L, 1L)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% Load Training Data\n",
    "data = loadmat('ex3data1.mat');#% training data stored in arrays X, y\n",
    "data['X'].shape\n",
    "data['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m,n = data['X'].shape\n",
    "#% Randomly select 100 data points to display\n",
    "rand_indices = np.random.permutation(m)\n",
    "sel = data['X'][rand_indices[0:100], :];\n",
    "sel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def displayData(sel):\n",
    "    m,n = sel.shape\n",
    "    example_width = int(np.sqrt(n))\n",
    "    example_height = int(n/example_width)\n",
    "    display_rows = int(np.sqrt(m))\n",
    "    display_cols = int(np.sqrt(m))\n",
    "    pad = 1;\n",
    "    display_array = np.ones((pad + display_rows * (example_height + pad),  pad + display_cols * (example_width + pad)))\n",
    "    curr_ex = 0;\n",
    "    for j in range(display_rows):\n",
    "        for i in range(display_cols):\n",
    "            #% Copy the patch\n",
    "            #% Get the max value of the patch\n",
    "            #max_val = max(abs(X(curr_ex, :)));\n",
    "            array_temp = (sel[curr_ex,:].reshape((example_height,example_width))).T;\n",
    "            u1=pad + j * (example_height + pad);\n",
    "            u2=pad + j * (example_height + pad)+example_height;\n",
    "            v1=pad + i * (example_width + pad);\n",
    "            v2=pad + i * (example_width + pad)+example_width;\n",
    "            display_array[u1:u2,v1:v2] = array_temp\n",
    "            curr_ex = curr_ex + 1;\n",
    "    return display_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_array = displayData(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(display_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% ============ Part 2a: Vectorize Logistic Regression ============\n",
    "%  In this part of the exercise, you will reuse your logistic regression\n",
    "%  code from the last exercise. You task here is to make sure that your\n",
    "%  regularized logistic regression implementation is vectorized. After\n",
    "%  that, you will implement one-vs-all classification for the handwritten\n",
    "%  digit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    #%SIGMOID Compute sigmoid function\n",
    "    #%g = SIGMOID(z) computes the sigmoid of z.\n",
    "\n",
    "    #% You need to return the following variables correctly \n",
    "    g = np.matrix(np.zeros(np.shape(z)));\n",
    "\n",
    "#% ====================== YOUR CODE HERE ======================\n",
    "#% Instructions: Compute the sigmoid of each value of z (z can be a matrix,vector or scalar).\n",
    "    g = np.divide(1,(1+np.exp(-z)));    \n",
    "    return g\n",
    "\n",
    "def lrCostFunction(theta, X, y, lamb):\n",
    "    #%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization\n",
    "    #%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using\n",
    "    #%   theta as the parameter for regularized logistic regression and the\n",
    "    #%   gradient of the cost w.r.t. to the parameters. \n",
    "    #% Initialize some useful values\n",
    "    m=len(y);\n",
    "    J = 0;\n",
    "    grad = np.zeros(theta.shape);\n",
    "    n = theta.shape[0];\n",
    "    if len(np.shape(theta))==1:\n",
    "        theta = np.transpose(np.matrix(theta)); \n",
    "    #Compute the cost of a particular choice of theta.\n",
    "    #You should set J to the cost.\n",
    "    #Compute the partial derivatives and set grad to the partial \n",
    "    #derivatives of the cost w.r.t. each parameter in theta\n",
    "    H=sigmoid(X*theta);\n",
    "    #J = sum((-y.*log(H)-(1-y).*log(1-H)))/m+lambda/2/m*sum(theta(2:n).^2);\n",
    "    J = np.sum(np.multiply(-y,np.log(H))-np.multiply((1-y),np.log(1-H)))/m \\\n",
    "        +np.sum(np.power(theta[1:n],2))*lamb/2.0/m;\n",
    "    grad=np.transpose(X)*(H-y)/m+lamb/m*theta;\n",
    "    grad[0] = grad[0]-lamb/m*theta[0];\n",
    "    #grad=1\n",
    "    return J,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_t = np.matrix([-2, -1, 1, 2]).T;\n",
    "X_t = np.matrix(np.ones((5,1)))\n",
    "tt = ((np.matrix(range(15)).reshape((3,5))).T+1.0)/10.0\n",
    "X_t = np.append(X_t,tt,axis=1)\n",
    "y_t = np.matrix([1,0,1,0,1]).T\n",
    "       \n",
    "lambda_t = 3.0;\n",
    "J,grad = lrCostFunction(theta_t, X_t, y_t, lambda_t);\n",
    "print('\\nCost: %f' %J);\n",
    "print('Expected cost: 2.534819');\n",
    "print('Gradients:');\n",
    "for i in range(len(grad)): print 'grad=%.4f  ' %(grad[i])\n",
    "print('Expected gradients:\\n');\n",
    "print(' 0.146561\\n -0.548558\\n 0.724722\\n 1.398003');\n",
    "print('Program paused. Press enter to continue.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, num_labels, lamb):\n",
    "#%ONEVSALL trains multiple logistic regression classifiers and returns all\n",
    "#%the classifiers in a matrix all_theta, where the i-th row of all_theta \n",
    "#%corresponds to the classifier for label i\n",
    "#%   [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels\n",
    "#%   logistic regression classifiers and returns each of these classifiers\n",
    "#%   in a matrix all_theta, where the i-th row of all_theta corresponds \n",
    "#%   to the classifier for label i\n",
    "#% Some useful variables\n",
    "    m,n = X.shape\n",
    "\n",
    "#% You need to return the following variables correctly \n",
    "    all_theta = np.zeros((num_labels, n + 1));\n",
    "\n",
    "#% Add ones to the X data matrix\n",
    "    X = np.c_[np.ones(m), X];\n",
    "    optimal_theta = np.zeros((num_labels,n+1))\n",
    "#% ====================== YOUR CODE HERE ======================\n",
    "#% Instructions: You should complete the following code to train num_labels\n",
    "#%               logistic regression classifiers with regularization\n",
    "#%               parameter lambda. \n",
    "#%\n",
    "#% Hint: theta[:] will return a column vector.\n",
    "#% Hint: You can use y == c to obtain a vector of 1's and 0's that tell you\n",
    "#%       whether the ground truth is true/false for this class.\n",
    "#% Note: For this assignment, we recommend using fmincg to optimize the cost\n",
    "#%       function. It is okay to use a for-loop (for c = 1:num_labels) to\n",
    "#%       loop over the different classes.\n",
    "#%\n",
    "#%       fmincg works similarly to fminunc, but is more efficient when we\n",
    "#%       are dealing with large number of parameters.\n",
    "\n",
    "    for c in range(num_labels):  \n",
    "         #% Set Initial theta\n",
    "        initial_theta = np.zeros((n + 1, 1)); \n",
    "        yy = np.matrix(y==c+1)*1\n",
    "         #% Set options for fminunc\n",
    "        #options = optimset('GradObj', 'on', 'MaxIter', 50);\n",
    "        Result = op.minimize(fun = lrCostFunction, \n",
    "                    x0 = initial_theta, \n",
    "                    args = (X, yy,lamb),\n",
    "                    method = 'TNC',\n",
    "                    jac = True);\n",
    "        optimal_theta[c,:] = np.ravel(np.transpose(np.matrix(Result.x)))\n",
    "    return optimal_theta      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training One-vs-All Logistic Regression...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#%% ============ Part 2b: One-vs-All Training ============\n",
    "print('Training One-vs-All Logistic Regression...')\n",
    "X=data['X']\n",
    "y=data['y']\n",
    "lamb = 0;\n",
    "all_theta = oneVsAll(X, y, num_labels, lamb);\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictOneVsAll(all_theta,X):\n",
    "    #%PREDICT Predict the label for a trained one-vs-all classifier. The labels \n",
    "    #%are in the range 1..K, where K = size(all_theta, 1). \n",
    "    #%  p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions\n",
    "    #%  for each example in the matrix X. Note that X contains the examples in\n",
    "    #%  rows. all_theta is a matrix where the i-th row is a trained logistic\n",
    "    #%  regression theta vector for the i-th class. You should set p to a vector\n",
    "    #%  of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2\n",
    "    #%  for 4 examples) \n",
    "    m,n = X.shape\n",
    "    num_labels,nn = all_theta.shape;\n",
    "    p = np.zeros((m, 1));\n",
    "    X = np.c_[np.ones(m), X];\n",
    "    #% Hint: This code can be done all vectorized using the max function.\n",
    "#%       In particular, the max function can also return the index of the \n",
    "#%       max element, for more information see 'help max'. If your examples \n",
    "#%       are in rows, then, you can use max(A, [], 2) to obtain the max \n",
    "#%       for each row.\n",
    "       \n",
    "    A=sigmoid(X*(np.matrix(all_theta.T)))\n",
    "    p=np.argmax(A,1)+1;\n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 97.600000\n"
     ]
    }
   ],
   "source": [
    "#%% ================ Part 3: Predict for One-Vs-All ================\n",
    "pred = predictOneVsAll(all_theta,X);\n",
    "accuracy = np.mean((pred==y)*1)*100\n",
    "print('Training Set Accuracy: %f'% accuracy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.63159614e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.83430039e-02,   2.85731475e-07,   0.00000000e+00],\n",
       "       [ -5.71590195e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          6.55275806e-02,  -6.83928011e-03,   0.00000000e+00],\n",
       "       [ -8.66581914e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -2.72550208e-04,  -1.16072803e-06,   0.00000000e+00],\n",
       "       ..., \n",
       "       [ -1.29089662e+01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -6.81939212e+00,   7.87510982e-01,   0.00000000e+00],\n",
       "       [ -8.61088375e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -1.99280663e-01,   1.08948233e-02,   0.00000000e+00],\n",
       "       [ -9.28206159e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          9.00830871e-04,   4.01741201e-05,   0.00000000e+00]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
