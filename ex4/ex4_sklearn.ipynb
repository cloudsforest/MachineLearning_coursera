{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.io import loadmat  \n",
    "import seaborn as sbs\n",
    "import matplotlib.image as mpimg\n",
    "import scipy.optimize as op\n",
    "from scipy.optimize import minimize\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  #% 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   #% 25 hidden units\n",
    "num_labels = 10; #% 10 labels, from 1 to 10 (note that we have mapped \"0\" to label 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = loadmat('ex4data1.mat');\n",
    "X=data['X']\n",
    "y=data['y']\n",
    "m,n = X.shape\n",
    "#% Randomly select 100 data points to display\n",
    "rand_indices = np.random.permutation(m)\n",
    "train_number = int(m/3.0*2.0)\n",
    "#X_train = X[rand_indices[0:train_number], :];\n",
    "#y_train = y[rand_indices[0:train_number], :];\n",
    "#X_test = X[rand_indices[train_number:], :];\n",
    "#y_test = y[rand_indices[train_number:], :];\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) #split data to train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The neural network may have difficulty converging before the maximum number of iterations allowed \n",
    "if the data is not normalized. Multi-layer Perceptron is sensitive to feature scaling, so it is \n",
    "highly recommended to scale your data. Note that you must apply the same scaling to the test set for \n",
    "meaningful results. There are a lot of different methods for normalization of data, we will use the \n",
    "built-in StandardScaler for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "#scaler.fit(X_train)\n",
    "#StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "#X_train = scaler.transform(X_train)\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.25, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 50, 50, 50), learning_rate='adaptive',\n",
       "       learning_rate_init=0.01, max_iter=50000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(activation='relu',solver='adam',alpha=0.25,hidden_layer_sizes=(100,50,50,50),learning_rate='adaptive',\n",
    "                    learning_rate_init=0.01,max_iter=50000,tol=0.0001,verbose=False)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[109   0   1   0   0   0   2   2   0   0]\n",
      " [  0 106   1   2   0   3   2  11   0   1]\n",
      " [  0   0 113   0   0   0   1   1   2   0]\n",
      " [  1   0   0 125   0   3   0   2   0   1]\n",
      " [  0   0   1   2 110   0   0   3   1   1]\n",
      " [  0   0   0   0   1 124   0   0   0   3]\n",
      " [  0   1   0   1   0   0 121   0   1   1]\n",
      " [  0   0   0   1   0   0   1 129   1   1]\n",
      " [  0   0   0   4   0   0   4   1 126   0]\n",
      " [  0   0   1   0   0   0   0   1   0 120]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.99      0.96      0.97       114\n",
      "          2       0.99      0.84      0.91       126\n",
      "          3       0.97      0.97      0.97       117\n",
      "          4       0.93      0.95      0.94       132\n",
      "          5       0.99      0.93      0.96       118\n",
      "          6       0.95      0.97      0.96       128\n",
      "          7       0.92      0.97      0.95       125\n",
      "          8       0.86      0.97      0.91       133\n",
      "          9       0.96      0.93      0.95       135\n",
      "         10       0.94      0.98      0.96       122\n",
      "\n",
      "avg / total       0.95      0.95      0.95      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MLPClassifier.score of MLPClassifier(activation='relu', alpha=0.25, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 50, 50, 50), learning_rate='adaptive',\n",
       "       learning_rate_init=0.01, max_iter=50000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)>"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
